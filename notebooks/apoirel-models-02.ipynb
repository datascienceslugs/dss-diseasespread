{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some baseline models (2)\n",
    "*Anders Poirel - 13-02-2020*\n",
    "\n",
    "\n",
    "In this notebook I will build a few simple models to get a feel for the dataset, and ideas on directions to improve the competition score in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Note**: I realized I used *median* absolute error instead of mean absolute error in a previous version of this notebook, which explain why my CV scores were so far from the test set scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/raw/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(join(DATA_PATH, 'dengue_features_test.csv'))\n",
    "X_train = pd.read_csv(join(DATA_PATH, 'dengue_features_train.csv'))\n",
    "y_train = pd.read_csv(join(DATA_PATH, 'dengue_labels_train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train, columns = ['city'], drop_first = True)\n",
    "X_test = pd.get_dummies(X_test, columns = ['city'], drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unecessary / correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('week_start_date', axis = 1)\n",
    "X_test = X_test.drop('week_start_date', axis = 1)\n",
    "y_train = y_train['total_cases']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now can move on to building a few simple models. We will be evaluating on Mean Absolute Error (MAE) since this is how the competition will be scored.\n",
    "\n",
    "**Note**: at first I observer unusually good MAE scores on CV, which turned out to be spurious since I was using a normal cross-validation split and indeed fitting the model to future data, so make sure to use a `TimeSeriesSplit` in your CV ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.model_selection import (cross_validate, TimeSeriesSplit, \n",
    "                                     RandomizedSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy = 'median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = Pipeline([\n",
    "    ('impute_m', imputer),\n",
    "    ('ols', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = cross_validate(\n",
    "    estimator = ols_model,\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    cv = TimeSeriesSplit(n_splits = 10),\n",
    "    scoring = 'neg_mean_absolute_error',\n",
    "    n_jobs = -1\n",
    ")\n",
    "ols_score = np.mean(cv_res['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-35.796794518364635"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this overfitting? Let's add some complexity penalties to find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in the first iteration of this test, I forgot to scale features (!)\n",
    "resulting in an MAE of ~26.4. Scaling improved this a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('impute_m', imputer),\n",
    "    ('en', ElasticNet())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = cross_validate(\n",
    "    estimator = en_model,\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    cv = TimeSeriesSplit(n_splits = 10),\n",
    "    scoring = 'neg_mean_absolute_error',\n",
    "    n_jobs = -1\n",
    ")\n",
    "en_score = np.mean(cv_res['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28.03091292318651"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at where this model is making errors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cranking up the penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-26.557742385277624"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enmore_model = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('impute_m', imputer),\n",
    "    ('en', ElasticNet(alpha = 2))\n",
    "])\n",
    "cv_res = cross_validate(\n",
    "    estimator = enmore_model,\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    cv = TimeSeriesSplit(n_splits = 10),\n",
    "    scoring = 'neg_mean_absolute_error',\n",
    "    n_jobs = -1\n",
    ")\n",
    "enmore_score = np.mean(cv_res['test_score'])\n",
    "enmore_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doubling the penalty seems to give a tiny edge over the previous model. Manual testing shows that increasing it further makes things worse.\n",
    "\n",
    "It looks like raw regularization does not improve things by much, so let's try increasing model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernels for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model = Pipeline([\n",
    "    ('impute_m', imputer),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('poly_f', PolynomialFeatures(degree = 2)),\n",
    "    ('poly_reg', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = cross_validate(\n",
    "    estimator = poly_model,\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    cv = TimeSeriesSplit(n_splits = 10),\n",
    "    scoring = 'neg_median_absolute_error',\n",
    "    n_jobs = -1\n",
    ")\n",
    "poly_score = np.mean(cv_res['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-155.23319488400816"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With complexity penalties\n",
    "\n",
    "Default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model_2 = Pipeline([\n",
    "    ('impute_m', imputer),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('poly_f', PolynomialFeatures(degree = 2)),\n",
    "    ('poly_reg', ElasticNet())\n",
    "])\n",
    "cv_res = cross_validate(\n",
    "    estimator = poly_model_2,\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    cv = TimeSeriesSplit(n_splits = 10),\n",
    "    scoring = 'neg_mean_absolute_error',\n",
    "    n_jobs = -1\n",
    ")\n",
    "poly_score_2 = np.mean(cv_res['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-39.77474083230882"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_score_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increased ($\\alpha$ = 5 was obtainied by trying manually all values in 1-10 range and 20, 50, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model_3 = Pipeline([\n",
    "    ('impute_m', imputer),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('poly_f', PolynomialFeatures(degree = 2)),\n",
    "    ('poly_reg', ElasticNet(alpha = 6))\n",
    "])\n",
    "cv_res = cross_validate(\n",
    "    estimator = poly_model_3,\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    cv = TimeSeriesSplit(n_splits = 10),\n",
    "    scoring = 'neg_mean_absolute_error',\n",
    "    n_jobs = -1\n",
    ")\n",
    "poly_score_3 = np.mean(cv_res['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-24.209824228207065"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_score_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally a big improvement!\n",
    "\n",
    "Testing with `degree = 3` worsens performance across the board. I did not bother testing with even higher degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-aways\n",
    "\n",
    "Adding an (untuned!) ElasticNet penalty improved performance quite a bit, (30.5 -> 23.4 MAE), while increasing the complexity of the model with a polynomial kernel decreased it (30.5 -> 155.2 MAE), which hints that we overfit easily and improvements in the model will more likely come from more regularization than model complexity - Or at least that any increase in complexity will need to be balanced big regularization)\n",
    "\n",
    "Things to try out in the future:\n",
    "- some pre-selection of features to eliminate redundant ones \n",
    "- model-based selection of features\n",
    "- looking at where the model is making errors and engineerign features refecting that if we see a trend\n",
    "- model-based feature imputation\n",
    "- splitting the model between two cities as the structure of the data is different between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model we found was a polynomial regression with deg. 2 kernels and a ElasticNet penalty $\\alpha = 5$, so I use this for our first submission on DrivenData:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(join(DATA_PATH, 'submission_format.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('impute_m',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='median',\n",
       "                               verbose=0)),\n",
       "                ('scale',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('poly_f',\n",
       "                 PolynomialFeatures(degree=2, include_bias=True,\n",
       "                                    interaction_only=False, order='C')),\n",
       "                ('poly_reg',\n",
       "                 ElasticNet(alpha=5, copy_X=True, fit_intercept=True,\n",
       "                            l1_ratio=0.5, max_iter=1000, normalize=False,\n",
       "                            positive=False, precompute=False, random_state=None,\n",
       "                            selection='cyclic', tol=0.0001,\n",
       "                            warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_model_3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = poly_model_3.predict(X_test)\n",
    "submission['total_cases'] = np.round(y_pred).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>total_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>iq</td>\n",
       "      <td>2013</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>iq</td>\n",
       "      <td>2013</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>iq</td>\n",
       "      <td>2013</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>iq</td>\n",
       "      <td>2013</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>iq</td>\n",
       "      <td>2013</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    city  year  weekofyear  total_cases\n",
       "0     sj  2008          18           23\n",
       "1     sj  2008          19           22\n",
       "2     sj  2008          20           27\n",
       "3     sj  2008          21           26\n",
       "4     sj  2008          22           26\n",
       "..   ...   ...         ...          ...\n",
       "411   iq  2013          22           16\n",
       "412   iq  2013          23           13\n",
       "413   iq  2013          24           14\n",
       "414   iq  2013          25           14\n",
       "415   iq  2013          26           14\n",
       "\n",
       "[416 rows x 4 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../models/baseline.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
